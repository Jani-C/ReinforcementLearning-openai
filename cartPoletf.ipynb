{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cartPoletf.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jani-C/ReinforcementLearning-openai/blob/master/cartPoletf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ5BgHcWId57",
        "colab_type": "code",
        "outputId": "f76de4e7-659a-42ff-b4e3-e8e5009f5c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "'''try:\n",
        "    xrange = xrange\n",
        "except:\n",
        "    xrange = range'''\n",
        "    \n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "gamma = 0.99\n",
        "\n",
        "def discount_rewards(r):\n",
        "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "    discounted_r = np.zeros_like(r)\n",
        "    running_add = 0\n",
        "    for t in reversed(range(0, r.size)):\n",
        "        running_add = running_add * gamma + r[t]\n",
        "        discounted_r[t] = running_add\n",
        "    return discounted_r\n",
        "\n",
        "class agent():\n",
        "    def __init__(self, lr, s_size,a_size,h_size):\n",
        "        #These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
        "        self.state_in= tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
        "        hidden = slim.fully_connected(self.state_in,h_size,biases_initializer=None,activation_fn=tf.nn.relu)\n",
        "        self.output = slim.fully_connected(hidden,a_size,activation_fn=tf.nn.softmax,biases_initializer=None)\n",
        "        self.chosen_action = tf.argmax(self.output,1)\n",
        "\n",
        "        #The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
        "        #to compute the loss, and use it to update the network.\n",
        "        self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
        "        self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
        "        \n",
        "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
        "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
        "\n",
        "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
        "        \n",
        "        tvars = tf.trainable_variables()\n",
        "        self.gradient_holders = []\n",
        "        for idx,var in enumerate(tvars):\n",
        "            placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
        "            self.gradient_holders.append(placeholder)\n",
        "        \n",
        "        self.gradients = tf.gradients(self.loss,tvars)\n",
        "        \n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))\n",
        "\n",
        "tf.reset_default_graph() #Clear the Tensorflow graph.\n",
        "\n",
        "myAgent = agent(lr=1e-2,s_size=4,a_size=2,h_size=8) #Load the agent.\n",
        "\n",
        "total_episodes = 5000 #Set total number of episodes to train agent on.\n",
        "max_ep = 999\n",
        "update_frequency = 5\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the tensorflow graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    i = 0\n",
        "    total_reward = []\n",
        "    total_length = []\n",
        "        \n",
        "    gradBuffer = sess.run(tf.trainable_variables())\n",
        "    for ix,grad in enumerate(gradBuffer):\n",
        "        gradBuffer[ix] = grad * 0\n",
        "        \n",
        "    while i < total_episodes:\n",
        "        s = env.reset()\n",
        "        running_reward = 0\n",
        "        ep_history = []\n",
        "        for j in range(max_ep):\n",
        "            #Probabilistically pick an action given our network outputs.\n",
        "            a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[s]})\n",
        "            a = np.random.choice(a_dist[0],p=a_dist[0])\n",
        "            a = np.argmax(a_dist == a)\n",
        "\n",
        "            s1,r,d,_ = env.step(a) #Get our reward for taking an action given a bandit.\n",
        "            ep_history.append([s,a,r,s1])\n",
        "            s = s1\n",
        "            running_reward += r\n",
        "            if d == True:\n",
        "                #Update the network.\n",
        "                ep_history = np.array(ep_history)\n",
        "                ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
        "                feed_dict={myAgent.reward_holder:ep_history[:,2],\n",
        "                        myAgent.action_holder:ep_history[:,1],myAgent.state_in:np.vstack(ep_history[:,0])}\n",
        "                grads = sess.run(myAgent.gradients, feed_dict=feed_dict)\n",
        "                for idx,grad in enumerate(grads):\n",
        "                    gradBuffer[idx] += grad\n",
        "\n",
        "                if i % update_frequency == 0 and i != 0:\n",
        "                    feed_dict= dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
        "                    _ = sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
        "                    for ix,grad in enumerate(gradBuffer):\n",
        "                        gradBuffer[ix] = grad * 0\n",
        "                \n",
        "                total_reward.append(running_reward)\n",
        "                total_length.append(j)\n",
        "                break\n",
        "\n",
        "        \n",
        "            #Update our running tally of scores.\n",
        "        if i % 100 == 0:\n",
        "            print(np.mean(total_reward[-100:]))\n",
        "        i += 1\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "21.0\n",
            "37.6\n",
            "38.99\n",
            "46.67\n",
            "51.67\n",
            "66.48\n",
            "79.58\n",
            "116.85\n",
            "151.96\n",
            "175.73\n",
            "183.2\n",
            "185.88\n",
            "188.26\n",
            "196.9\n",
            "191.4\n",
            "194.68\n",
            "184.22\n",
            "181.6\n",
            "177.85\n",
            "192.97\n",
            "198.09\n",
            "195.39\n",
            "196.16\n",
            "196.45\n",
            "196.72\n",
            "194.02\n",
            "190.8\n",
            "190.14\n",
            "191.31\n",
            "194.33\n",
            "195.83\n",
            "199.28\n",
            "199.91\n",
            "200.0\n",
            "199.55\n",
            "199.36\n",
            "200.0\n",
            "200.0\n",
            "200.0\n",
            "200.0\n",
            "200.0\n",
            "200.0\n",
            "199.36\n",
            "197.01\n",
            "197.01\n",
            "194.87\n",
            "189.8\n",
            "187.89\n",
            "185.27\n",
            "186.48\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}